---
title: What is an Evaluation?
description: Understanding how time series prediction models are evaluated
order: 2
category: Basics
---

## What is an Evaluation?

An **evaluation** tests how well a predictive model performs using your historical data. Before you can use a model to forecast future disease cases, you first need to know: *can this model actually make accurate predictions?*

This is what an evaluation does.

Let's say that this is your disease data for the last 2 years, aggregated by month. This shows a clear seasonal pattern, with a peak in January.

<PredictionAnimation step={0} />

From here, we want to know whether this is enough data to train a model that can make accurate predictions. 
To answer this question, we can create an evaluation in the modeling app. An evaluation takes in a set of parameters defined by the model you are using, and splits the data into training and test sets automatically. 
It then generates a range of predictions for the test set, and compares them to the actual values.

> See how to do this in the [Creating an Evaluation](#/guides/creating-an-evaluation) guide.

After running a successful evaluation, it will give you something like this:

<PredictionAnimation step={1} />

The output of the model is shown as a range of predictions, with the median prediction as a solid line. The outer range is the 80% prediction interval, and the mid range is the 50% prediction interval.

What we're actually interested in is how this model performed over multiple time periods. Try to click the buttons below to see the model performance over time and use the tooltips to see the actual values.

<PredictionAnimation showButtons={true} />

### Is this a good model?

The model evaluation definitely shows that the model has picked up on some sort of trend, as the median prediction is very close to the actual cases. The trajectory of the median prediction also seems to follow the correct trajectory.
This is a good sign.

However, we can also see that the model has a high uncertainty in its predictions, especially for the last month. There are many factors that can contribute to this, such as:
- The model is not trained on enough data
- The model parameters needs to be tuned or adjusted
- The quality of your data is not good enough

Whether this model is good enough for your use case depends on your specific requirements, and usually involves a combination of domain knowledge and data analysis.
Evaluating a model is an iterative process. If you are not satisfied with the performance of the model, the next step would be to tune any parameters or adjust the model to reduce the uncertainty.
In the next guide, we will see how to iterate on a model to improve its performance.
